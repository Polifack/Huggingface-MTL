{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Sequence\n",
    "from datasets import ClassLabel\n",
    "\n",
    "def load_conll_dataset(train_path, dev_path, test_path, token_idx, label_idx):    \n",
    "    def read_conll_file(file_path, token_idx, label_idx):        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            sentences = [[]]\n",
    "            for line in f:\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                \n",
    "                line = line.strip()\n",
    "                \n",
    "                if line:\n",
    "                    split = line.split('\\t')\n",
    "                    sentences[-1].append((split[token_idx], split[label_idx]))\n",
    "                \n",
    "                else:\n",
    "                    if sentences[-1]:\n",
    "                        sentences.append([])\n",
    "            \n",
    "            if not sentences[-1]:\n",
    "                sentences.pop()\n",
    "\n",
    "        # Convert sentences to Hugging Face Dataset format\n",
    "        dataset = {\n",
    "            \"tokens\": [[token for token, label in sentence] for sentence in sentences],\n",
    "            \"target\": [[label for token, label in sentence] for sentence in sentences],\n",
    "        }\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    train_dset = read_conll_file(train_path, token_idx, label_idx)\n",
    "    dev_dset = read_conll_file(dev_path, token_idx, label_idx)\n",
    "    test_dset = read_conll_file(test_path, token_idx, label_idx)\n",
    "\n",
    "    # Get all possible labels and cast to ClassLabel\n",
    "    label_set = set()\n",
    "    for dset in [train_dset, dev_dset, test_dset]:\n",
    "        for labels in dset[\"target\"]:\n",
    "            label_set.update(labels)\n",
    "    label_names = sorted(list(label_set))\n",
    "    \n",
    "    train_dset = datasets.Dataset.from_dict(train_dset)\n",
    "    train_dset = train_dset.cast_column(\"target\", Sequence(ClassLabel(names=label_names)))\n",
    "\n",
    "    dev_dset = datasets.Dataset.from_dict(dev_dset)\n",
    "    dev_dset = dev_dset.cast_column(\"target\", Sequence(ClassLabel(names=label_names)))\n",
    "\n",
    "    test_dset = datasets.Dataset.from_dict(test_dset)\n",
    "    test_dset = test_dset.cast_column(\"target\", Sequence(ClassLabel(names=label_names)))\n",
    "    \n",
    "    # Convert to Hugging Face DatasetDict format\n",
    "    dataset = datasets.DatasetDict({\n",
    "            \"train\": train_dset,\n",
    "            \"validation\": dev_dset,\n",
    "            \"test\": test_dset\n",
    "        })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# raw_dataset = load_conll_dataset(\"./data/train.conllu\", \"./data/dev.conllu\", \"./data/test.conllu\", 1, 3)\n",
    "# print(raw_dataset[\"train\"][0][\"tokens\"])\n",
    "# print(raw_dataset[\"train\"][0][\"target\"])\n",
    "# print(raw_dataset[\"train\"].features[\"target\"].feature.names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should take as input a .json file with the training info. The input files will be in columns separated by \\t. In the json file we will specify the 'target' columns. If more than one target column, then we will duplicate the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hfmtl.tasks.sequence_classification import SequenceClassification\n",
    "from hfmtl.tasks.token_classification import TokenClassification\n",
    "from hfmtl.utils import *\n",
    "from hfmtl.models import *\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import easydict\n",
    "from frozendict import frozendict\n",
    "import json\n",
    "\n",
    "# read train_config.json as easydict\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    args = easydict.EasyDict(json.load(f))\n",
    "\n",
    "tasks = []\n",
    "for task in args.tasks:\n",
    "    if task.task_type == \"token_classification\":\n",
    "        tasks.append(\n",
    "            TokenClassification(\n",
    "                dataset = load_conll_dataset(task.train_file, task.eval_file, task.test_file, task.tokens_idx, l_idx),\n",
    "                name = task.task_name,\n",
    "                y = [\"target\"] if len(task.label_idx)==1 else [f\"target_{i}\" for i in range(len(task.label_idx))],\n",
    "                tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=args.max_seq_length, truncation=True)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    elif task.type == \"sequence_classification\":\n",
    "        tasks.append(\n",
    "            SequenceClassification(\n",
    "                dataset = load_conll_dataset(task.train_file, task.eval_file, task.test_file, task.tokens_idx, l_idx),\n",
    "                name = task.name,\n",
    "                y = [\"target\"] if len(task.label_idx)==1 else [f\"target_{i}\" for i in range(len(task.label_idx))],\n",
    "                tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=args.max_seq_length, truncation=True)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "pretrained = 'xlm-roberta-base'\n",
    "model = MultiTaskModel(pretrained, tasks)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = args.output_dir,\n",
    "    num_train_epochs = args.num_train_epochs,\n",
    "    learning_rate = args.learning_rate,\n",
    "    evaluation_strategy = args.logging_strategy,\n",
    "    save_strategy = args.logging_strategy,\n",
    "    weight_decay = args.weight_decay,\n",
    ")\n",
    "trainer = MultiTaskTrainer(\n",
    "    model = model,\n",
    "    tasks = tasks,\n",
    "    args = training_args,\n",
    "    train_dataset = model.train_dataset,\n",
    "    eval_dataset = model.eval_dataset,\n",
    "    compute_metrics = None,\n",
    "    tokenizer = model.tokenizer\n",
    ")\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
