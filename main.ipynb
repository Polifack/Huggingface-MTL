{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485bf127ea664f94b1a5ed709fcfd0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/12543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d669d3793cb34d00bf242461f799e1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d653e9773bb403090dd965b76c90e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
      "[11, 12, 11, 12, 0, 7, 15, 11, 11, 11, 12, 11, 12, 5, 7, 1, 5, 7, 1, 5, 7, 1, 11, 12, 1, 5, 0, 7, 12]\n",
      "['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', '_']\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Sequence\n",
    "from datasets import ClassLabel\n",
    "\n",
    "def load_conll_dataset(train_path, dev_path, test_path, token_idx, label_idx):    \n",
    "    def read_conll_file(file_path, token_idx, label_idx):        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            sentences = [[]]\n",
    "            for line in f:\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                \n",
    "                line = line.strip()\n",
    "                \n",
    "                if line:\n",
    "                    split = line.split('\\t')\n",
    "                    sentences[-1].append((split[token_idx], split[label_idx]))\n",
    "                \n",
    "                else:\n",
    "                    if sentences[-1]:\n",
    "                        sentences.append([])\n",
    "            \n",
    "            if not sentences[-1]:\n",
    "                sentences.pop()\n",
    "\n",
    "        # Convert sentences to Hugging Face Dataset format\n",
    "        dataset = {\n",
    "            \"tokens\": [[token for token, label in sentence] for sentence in sentences],\n",
    "            \"target\": [[label for token, label in sentence] for sentence in sentences],\n",
    "        }\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    train_dset = read_conll_file(train_path, token_idx, label_idx)\n",
    "    dev_dset = read_conll_file(dev_path, token_idx, label_idx)\n",
    "    test_dset = read_conll_file(test_path, token_idx, label_idx)\n",
    "\n",
    "    # Get all possible labels and cast to ClassLabel\n",
    "    label_set = set()\n",
    "    for dset in [train_dset, dev_dset, test_dset]:\n",
    "        for labels in dset[\"target\"]:\n",
    "            label_set.update(labels)\n",
    "    label_names = sorted(list(label_set))\n",
    "    \n",
    "    train_dset = datasets.Dataset.from_dict(train_dset)\n",
    "    train_dset = train_dset.cast_column(\"target\", Sequence(ClassLabel(names=label_names)))\n",
    "\n",
    "    dev_dset = datasets.Dataset.from_dict(dev_dset)\n",
    "    dev_dset = dev_dset.cast_column(\"target\", Sequence(ClassLabel(names=label_names)))\n",
    "\n",
    "    test_dset = datasets.Dataset.from_dict(test_dset)\n",
    "    test_dset = test_dset.cast_column(\"target\", Sequence(ClassLabel(names=label_names)))\n",
    "    \n",
    "    # Convert to Hugging Face DatasetDict format\n",
    "    dataset = datasets.DatasetDict({\n",
    "            \"train\": train_dset,\n",
    "            \"validation\": dev_dset,\n",
    "            \"test\": test_dset\n",
    "        })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "raw_dataset = load_conll_dataset(\"./data/train.conllu\", \"./data/dev.conllu\", \"./data/test.conllu\", 1, 3)\n",
    "print(raw_dataset[\"train\"][0][\"tokens\"])\n",
    "print(raw_dataset[\"train\"][0][\"target\"])\n",
    "print(raw_dataset[\"train\"].features[\"target\"].feature.names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should take as input a .json file with the training info. The input files will be in columns separated by \\t. In the json file we will specify the 'target' columns. If more than one target column, then we will duplicate the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05de4c95d41149bdb01bd9c0dced5c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/12543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8d8ede05d04790bba778e4537d4dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aadd7149be54478a8fd84f4a3db64d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'t'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m task\u001b[39m.\u001b[39mtask_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[39mfor\u001b[39;00m l_idx \u001b[39min\u001b[39;00m task\u001b[39m.\u001b[39mlabel_idx:\n\u001b[1;32m     18\u001b[0m         tasks\u001b[39m.\u001b[39mappend(\n\u001b[0;32m---> 19\u001b[0m             TokenClassification(\n\u001b[1;32m     20\u001b[0m                 dataset \u001b[39m=\u001b[39;49m load_conll_dataset(task\u001b[39m.\u001b[39;49mtrain_file, task\u001b[39m.\u001b[39;49meval_file, task\u001b[39m.\u001b[39;49mtest_file, task\u001b[39m.\u001b[39;49mtokens_idx, l_idx),\n\u001b[1;32m     21\u001b[0m                 name \u001b[39m=\u001b[39;49m task\u001b[39m.\u001b[39;49mtask_name,\n\u001b[1;32m     22\u001b[0m                 tokenizer_kwargs \u001b[39m=\u001b[39;49m frozendict(padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mmax_seq_length, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     23\u001b[0m             )\n\u001b[1;32m     24\u001b[0m         )\n\u001b[1;32m     26\u001b[0m \u001b[39melif\u001b[39;00m task\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msequence_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     27\u001b[0m     \u001b[39mfor\u001b[39;00m l_idx \u001b[39min\u001b[39;00m task\u001b[39m.\u001b[39mlabel_idx:\n",
      "File \u001b[0;32m<string>:12\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, name, dataset, metric, main_split, tokens, y, num_labels, label_names, tokenizer_kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Huggingface-MTL/hfmtl/tasks/token_classification.py:47\u001b[0m, in \u001b[0;36mTokenClassification.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels  \u001b[39m=\u001b[39m {}\n\u001b[1;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my:\n\u001b[0;32m---> 47\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain_split]\u001b[39m.\u001b[39;49mfeatures[y]\n\u001b[1;32m     48\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels[y] \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mnum_classes\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_names[y] \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mnames \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mnames \u001b[39melse\u001b[39;00m [\u001b[39mNone\u001b[39;00m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 't'"
     ]
    }
   ],
   "source": [
    "# from hfmtl.tasks.sequence_classification import SequenceClassification\n",
    "from hfmtl.tasks.token_classification import TokenClassification\n",
    "from hfmtl.utils import *\n",
    "from hfmtl.models import *\n",
    "\n",
    "import easydict\n",
    "from frozendict import frozendict\n",
    "import json\n",
    "\n",
    "# read train_config.json as easydict\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    args = easydict.EasyDict(json.load(f))\n",
    "\n",
    "tasks = []\n",
    "for task in args.tasks:\n",
    "    if task.task_type == \"token_classification\":\n",
    "        for l_idx in task.label_idx:\n",
    "            tasks.append(\n",
    "                TokenClassification(\n",
    "                    dataset = load_conll_dataset(task.train_file, task.eval_file, task.test_file, task.tokens_idx, l_idx),\n",
    "                    name = task.task_name,\n",
    "                    tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=args.max_seq_length, truncation=True)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    elif task.type == \"sequence_classification\":\n",
    "        for l_idx in task.label_idx:\n",
    "            tasks.append(\n",
    "                SequenceClassification(\n",
    "                    dataset = load_conll_dataset(task.train_file, task.eval_file, task.test_file, task.tokens_idx, l_idx),\n",
    "                    name = task.name,\n",
    "                    tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=args.max_seq_length, truncation=True)\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "# model   = Model(tasks, args) # list of models; by default, shared encoder, task-specific CLS token task-specific head \n",
    "# trainer = Trainer(model, tasks, args) # tasks are uniformly sampled by default\n",
    "\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
