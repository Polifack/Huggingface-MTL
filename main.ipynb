{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def load_conll_dataset(train_path, dev_path, test_path, token_idx, label_idx):\n",
    "    \n",
    "    def read_conll_file(file_path, token_idx, label_idx):\n",
    "\n",
    "        stop_point = 10000\n",
    "        counter    = 0\n",
    "        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            sentences = [[]]\n",
    "            for line in f:\n",
    "                if counter == stop_point:\n",
    "                    break\n",
    "                counter += 1\n",
    "                line = line.strip()\n",
    "                \n",
    "                if line:\n",
    "                    split = line.split('\\t')\n",
    "                    sentences[-1].append((split[token_idx], split[label_idx]))\n",
    "                \n",
    "                else:\n",
    "                    if sentences[-1]:\n",
    "                        sentences.append([])\n",
    "            \n",
    "            if not sentences[-1]:\n",
    "                sentences.pop()\n",
    "\n",
    "        # Convert sentences to Hugging Face Dataset format\n",
    "        dataset = {\n",
    "            \"tokens\": [[token for token, label in sentence] for sentence in sentences],\n",
    "            \"tags\": [[label for token, label in sentence] for sentence in sentences],\n",
    "        }\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def label_to_int(dataset, label_set):\n",
    "        label_to_id = {label: i for i, label in enumerate(label_set)}\n",
    "        dataset[\"tags\"] = [[label_to_id[label] for label in labels] for labels in dataset[\"tags\"]]\n",
    "        return dataset\n",
    "    \n",
    "    train_dset = read_conll_file(train_path, token_idx, label_idx)\n",
    "    dev_dset = read_conll_file(dev_path, token_idx, label_idx)\n",
    "    test_dset = read_conll_file(test_path, token_idx, label_idx)\n",
    "\n",
    "    # Get all possible labels\n",
    "    label_set = set()\n",
    "    for dset in [train_dset, dev_dset]:\n",
    "        for labels in dset[\"tags\"]:\n",
    "            label_set.update(labels)\n",
    "    \n",
    "    # labels to int\n",
    "    train_dset = label_to_int(train_dset, label_set)\n",
    "    dev_dset = label_to_int(dev_dset, label_set)\n",
    "    test_dset = label_to_int(test_dset, label_set)\n",
    "\n",
    "    \n",
    "    # Convert to Hugging Face DatasetDict format\n",
    "    dataset = datasets.DatasetDict({\n",
    "            \"train\": datasets.Dataset.from_dict(train_dset),\n",
    "            \"validation\": datasets.Dataset.from_dict(dev_dset),\n",
    "            \"test\": datasets.Dataset.from_dict(test_dset)\n",
    "        })\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should take as input a .json file with the training info. The input files will be in columns separated by \\t. In the json file we will specify the 'target' columns. If more than one target column, then we will duplicate the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tasks.sequence_classification import SequenceClassification\n",
    "from src.tasks.token_classification import TokenClassification\n",
    "from src.utils import *\n",
    "from src.models import *\n",
    "\n",
    "import easydict\n",
    "from frozendict import frozendict\n",
    "import json\n",
    "\n",
    "# read train_config.json as easydict\n",
    "with open(\"train_config.json\", \"r\") as f:\n",
    "    args = easydict.EasyDict(json.load(f))\n",
    "\n",
    "tasks = []\n",
    "for task in args.tasks:\n",
    "    if task.task_type == \"token_classification\":\n",
    "        for l_idx in task.label_idx:\n",
    "            tasks.append(\n",
    "                TokenClassification(\n",
    "                    dataset = load_conll_dataset(task.train_file, task.eval_file, task.test_file, task.tokens_idx, l_idx),\n",
    "                    name = task.task_name,\n",
    "                    tokens = \"tokens\",\n",
    "                    y = \"tags\",\n",
    "                    tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=args.max_seq_length, truncation=True)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    elif task.type == \"sequence_classification\":\n",
    "        for l_idx in task.label_idx:\n",
    "            tasks.append(\n",
    "                SequenceClassification(\n",
    "                    dataset = load_conll_dataset(task.train_file, task.eval_file, task.test_file, task.tokens_idx, l_idx),\n",
    "                    name = task.name,\n",
    "                    tokens = \"tokens\",\n",
    "                    y = \"tags\",\n",
    "                    tokenizer_kwargs = frozendict(padding=\"max_length\", max_length=args.max_seq_length, truncation=True)\n",
    "                )\n",
    "            )\n",
    "\n",
    "print(tasks)\n",
    "\n",
    "models = Model(tasks, args) # list of models; by default, shared encoder, task-specific CLS token task-specific head \n",
    "trainer = Trainer(models, tasks, args) # tasks are uniformly sampled by default\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
