==> Problems:

- per_device_train_batch_size is half of train_batch_size
- cant set train_batch_size in train_argments? why?
- it was fixed by forcing the number of gpus to 1. should check this in order to allow for multi-gpu processing
- could the mult-gpu functionality be restored? if so, how? (to do long term)

  ##  use pytorch gatherers from multi-core like DistributedTensorGatherer?
  ##  if so, where?



==> consideration for NER datasets for token classification tasks:
  # NER: if the label is B-XXX we change it to I-XXX
  # print(label)
  # if label % 2 == 1:
  #     label += 1

==> quantization:
  to quantizate to 8 bits we can use:
    
    model = deep_copy_cache(model_type.from_pretrained)(args.model_name, ignore_mismatched_sizes=True, load_in_8bit=True, device_map='auto', **nl)

  however it does not work

==> evaluation raises warning about labels not being in BIO format