==> multi-gpu problem:

 - per_device_train_batch_size is half of train_batch_size
 - cant set train_batch_size in train_argments? why?

 - it was fixed by FORCING the number of gpus to 1. should check this in order to allow for multi-gpu processing
 - could the mult-gpu functionality be restored? if so, how? (to do long term)

==> consideration for NER datasets for token classification tasks:
  - code:
    
    # NER: if the label is B-XXX we change it to I-XXX
    # print(label)
    # if label % 2 == 1:
    #     label += 1

==> quantization:
 - to quantizate to 8 bits we can use:
    
    # model = deep_copy_cache(model_type.from_pretrained)(args.model_name, ignore_mismatched_sizes=True, load_in_8bit=True, device_map='auto', **nl)
 
 - it does not work

==> check what this is about 
 - There were missing keys in the checkpoint model loaded: ['Z', 'shared_encoder.embeddings.position_ids', 'shared_encoder.embeddings.word_embeddings.weight',(...)]
 - Explanation:
    
    you're loading the bert-base-cased checkpoint (which is a checkpoint that was trained using a similar architecture to BertForPreTraining) in a BertForSequenceClassification model.

    This means that:

    The layers that BertForPreTraining has, but BertForSequenceClassification does not have will be discarded
    The layers that BertForSequenceClassification has but BertForPreTraining does not have will be randomly initialized.

==> multi-task prediction does not work; task i!=0 labels are wrong (why?)

 - only task_0 is being factorized?
    
    # label2id for task 0 {'10[_]ADJP': 0, '10[_]ADJP[+]QP': 1, '10[_]ADVP': 2, '10[_]NP': 3, '10[_]NP[+]QP': 4, '10[_]NP[_]NP': 5, '10[_]NP[_]NX': 6, '10[_]NP[_]VP': 7, '10[_]PP': 8, '10[_]PP[_]INTJ': 9, '10[_]PRN': 10, '10[_]QP': 11, '10[_]S': 12, '10[_]SBAR': 13, '10[_]SBAR[+]S[_]NP': 14, '10[_]SBAR[_]WHNP': 15, '10[_]S[+]VP': 16, '10[_]S[+]VP[_]NP': 17, '10[_]S[+]VP[_]PRT': 18, '10[_]S[_]NP': 19, '10[_]VP': 20, '10[_]VP[_]NP': 21, '11[_]ADJP': 22, '11[_]ADJP[+]QP': 23, '11[_]ADVP': 24, '11[_]ADVP[_]ADVP': 25, '11[_]NP': 26, '11[_]NP[+]QP': 27, '11[_]NP[_]NP': 28, '11[_]PP': 29, '11[_]PP[_]NP': 30, '11[_]QP': 31, '11[_]S': 32, '11[_]SBAR': 33, '11[_]SBAR[+]S': 34, '11[_]SBAR[+]S[+]VP': 35, '11[_]SBAR[_]WHADVP': 36, '11[_]SBAR[_]WHNP': 37, '11[_]S[+]VP': 38, '11[_]S[_]NP': 39, '11[_]UCP': 40, '11[_]VP': 41, '11[_]VP[_]ADVP': 42, '11[_]VP[_]NP': 43, '11[_]VP[_]PP': 44, '11[_]WHPP': 45, '12[_]ADJP': 46, '12[_]ADVP': 47, '12[_]CONJP': 48, '12[_]NP': 49, '12[_]NP[_]ADVP': 50, '12[_]NP[_]NP': 51, '12[_]PP': 52, '12[_]QP': 53, '12[_]S': 54, '12[_]SBAR[+]S': 55, '12[_]SBAR[+]S[+]VP': 56, '12[_]SBAR[_]WHNP': 57, '12[_]S[+]VP': 58, '12[_]S[+]VP[_]NP': 59, '12[_]S[+]VP[_]VP': 60, '12[_]S[_]NP': 61, '12[_]VP': 62, '12[_]VP[_]ADVP': 63, '12[_]VP[_]NP': 64, '13[_]ADVP': 65, '13[_]NP': 66, '13[_]NP[_]ADJP': 67, '13[_]NX[_]NP': 68, '13[_]PP': 69, '13[_]SBAR[+]S': 70, '13[_]SBAR[_]WHNP': 71, '13[_]S[+]VP': 72, '13[_]VP': 73, '13[_]VP[_]ADVP': 74, '13[_]VP[_]NP': 75, '13[_]VP[_]PRT': 76, '14[_]NP': 77, '14[_]NP[+]QP': 78, '14[_]NP[_]NP': 79, '14[_]PP': 80, '14[_]SBAR[_]WHNP': 81, '14[_]S[+]NP[_]NP': 82, '14[_]S[+]VP': 83, '14[_]VP': 84, '14[_]VP[_]ADVP': 85, '15[_]ADJP': 86, '15[_]NP': 87, '15[_]NP[+]QP': 88, '15[_]PP': 89, '15[_]PRN': 90, '15[_]SBAR': 91, '15[_]SBAR[+]S[+]VP': 92, '15[_]SBAR[_]WHNP': 93, '15[_]S[+]VP': 94, '15[_]S[+]VP[_]ADVP': 95, '15[_]VP': 96, '16[_]ADJP': 97, '16[_]NP': 98, '16[_]NP[+]QP': 99, '16[_]NP[_]NP': 100, '16[_]PP': 101, '16[_]S': 102, '16[_]S[+]VP': 103, '16[_]S[+]VP[_]ADVP': 104, '16[_]S[+]VP[_]NP': 105, '16[_]S[_]NP': 106, '16[_]VP': 107, '16[_]VP[_]NP': 108, '17[_]NP': 109, '17[_]PP': 110, '17[_]SBAR[_]WHADVP': 111, '17[_]SQ': 112, '17[_]SQ[_]NP': 113, '17[_]SQ[_]VP': 114, '17[_]S[+]VP': 115, '17[_]S[_]NP': 116, '17[_]VP': 117, '18[_]NP': 118, '18[_]PP': 119, '18[_]S': 120, '18[_]VP': 121, '18[_]VP[_]PRT': 122, '19[_]ADJP': 123, '19[_]NP': 124, '19[_]PP': 125, '1[_]FRAG': 126, '1[_]NP': 127, '1[_]S': 128, '1[_]SBARQ': 129, '1[_]SBARQ[_]ADJP': 130, '1[_]SBARQ[_]WHNP': 131, '1[_]SINV': 132, '1[_]SINV[_]ADVP': 133, '1[_]SINV[_]NP': 134, '1[_]SINV[_]PRT': 135, '1[_]SINV[_]VP': 136, '1[_]SQ': 137, '1[_]SQ[_]NP': 138, '1[_]SQ[_]VP': 139, '1[_]S[_]ADJP': 140, '1[_]S[_]ADVP': 141, '1[_]S[_]INTJ': 142, '1[_]S[_]NP': 143, '1[_]S[_]NP[+]NP': 144, '1[_]S[_]S[+]VP': 145, '1[_]S[_]VP': 146, '1[_]UCP': 147, '20[_]NP': 148, '20[_]PP': 149, '20[_]QP': 150, '21[_]NP': 151, '2[_]ADVP': 152, '2[_]ADVP[_]NP': 153, '2[_]NP': 154, '2[_]NP[_]NP': 155, '2[_]NP[_]NX': 156, '2[_]PP': 157, '2[_]PRN': 158, '2[_]PRN[_]VP': 159, '2[_]S': 160, '2[_]SBAR': 161, '2[_]SBAR[_]WHADVP': 162, '2[_]SBAR[_]WHNP': 163, '2[_]SINV': 164, '2[_]SINV[_]VP': 165, '2[_]SQ': 166, '2[_]SQ[_]ADVP': 167, '2[_]SQ[_]NP': 168, '2[_]S[_]ADJP': 169, '2[_]S[_]ADVP': 170, '2[_]S[_]NP': 171, '2[_]S[_]NP[+]NP': 172, '2[_]S[_]PRT': 173, '2[_]S[_]S[+]VP': 174, '2[_]S[_]VP': 175, '2[_]VP': 176, '2[_]VP[_]ADJP': 177, '2[_]VP[_]ADVP': 178, '2[_]VP[_]NP': 179, '2[_]VP[_]PRT': 180, '2[_]VP[_]S[+]VP': 181, '2[_]VP[_]VP': 182, '3[_]ADJP': 183, '3[_]ADVP': 184, '3[_]NAC': 185, '3[_]NP': 186, '3[_]NP[+]NP': 187, '3[_]NP[+]QP': 188, '3[_]NP[_]ADVP': 189, '3[_]NP[_]NP': 190, '3[_]NP[_]PP': 191, '3[_]NX': 192, '3[_]PP': 193, '3[_]PRN': 194, '3[_]PRN[_]VP': 195, '3[_]QP': 196, '3[_]S': 197, '3[_]SBAR': 198, '3[_]SBAR[_]VP': 199, '3[_]SBAR[_]WHADVP': 200, '3[_]SBAR[_]WHNP': 201, '3[_]SQ[+]VP': 202, '3[_]S[+]ADJP': 203, '3[_]S[+]VP': 204, '3[_]S[_]ADVP': 205, '3[_]S[_]NP': 206, '3[_]S[_]NP[+]NP': 207, '3[_]UCP': 208, '3[_]VP': 209, '3[_]VP[_]ADJP': 210, '3[_]VP[_]ADVP': 211, '3[_]VP[_]ADVP[+]ADVP': 212, '3[_]VP[_]NP': 213, '3[_]VP[_]PRT': 214, '3[_]VP[_]VP': 215, '4[_]ADJP': 216, '4[_]ADJP[+]QP': 217, '4[_]ADJP[_]VP': 218, '4[_]ADVP': 219, '4[_]NP': 220, '4[_]NP[+]NP': 221, '4[_]NP[_]NP': 222, '4[_]NP[_]VP': 223, '4[_]NX': 224, '4[_]PP': 225, '4[_]PP[_]ADVP': 226, '4[_]PRN': 227, '4[_]PRN[_]NP': 228, '4[_]QP': 229, '4[_]S': 230, '4[_]SBAR': 231, '4[_]SBAR[+]S': 232, '4[_]SBAR[+]S[_]NP': 233, '4[_]SBAR[+]S[_]NP[+]NP': 234, '4[_]SBAR[_]NP': 235, '4[_]SBAR[_]WHADVP': 236, '4[_]SBAR[_]WHNP': 237, '4[_]S[+]ADJP': 238, '4[_]S[+]VP': 239, '4[_]S[+]VP[_]ADVP': 240, '4[_]S[+]VP[_]NP': 241, '4[_]S[+]VP[_]PRT': 242, '4[_]S[_]ADJP': 243, '4[_]S[_]ADVP': 244, '4[_]S[_]NP': 245, '4[_]S[_]S[+]VP': 246, '4[_]UCP': 247, '4[_]VP': 248, '4[_]VP[+]VP': 249, '4[_]VP[+]VP[_]PRT': 250, '4[_]VP[_]NP': 251, '4[_]VP[_]PP': 252, '4[_]VP[_]PRT': 253, '4[_]VP[_]VP': 254, '4[_]WHNP': 255, '4[_]WHPP': 256, '5[_]ADJP': 257, '5[_]ADVP': 258, '5[_]NP': 259, '5[_]NP[+]NP': 260, '5[_]NP[_]ADJP': 261, '5[_]NP[_]NP': 262, '5[_]NP[_]PP': 263, '5[_]NP[_]VP': 264, '5[_]PP': 265, '5[_]PRN': 266, '5[_]PRN[_]VP': 267, '5[_]QP': 268, '5[_]S': 269, '5[_]SBAR': 270, '5[_]SBAR[+]S': 271, '5[_]SBAR[+]S[+]VP': 272, '5[_]SBAR[+]S[_]NP': 273, '5[_]SBAR[_]WHADVP': 274, '5[_]SBAR[_]WHNP': 275, '5[_]S[+]NP': 276, '5[_]S[+]VP': 277, '5[_]S[+]VP[_]ADVP': 278, '5[_]S[+]VP[_]NP': 279, '5[_]S[_]ADVP': 280, '5[_]S[_]NP': 281, '5[_]VP': 282, '5[_]VP[_]ADJP': 283, '5[_]VP[_]ADVP': 284, '5[_]VP[_]NP': 285, '5[_]VP[_]PRT': 286, '5[_]WHNP': 287, '5[_]WHPP': 288, '6[_]ADJP': 289, '6[_]ADVP': 290, '6[_]ADVP[_]NP': 291, '6[_]INTJ': 292, '6[_]LST': 293, '6[_]NP': 294, '6[_]NP[+]NP': 295, '6[_]NP[+]QP': 296, '6[_]NP[_]NP': 297, '6[_]NX': 298, '6[_]PP': 299, '6[_]PP[_]ADVP': 300, '6[_]PRN': 301, '6[_]QP': 302, '6[_]S': 303, '6[_]SBAR': 304, '6[_]SBAR[+]S': 305, '6[_]SBAR[+]S[+]VP': 306, '6[_]SBAR[+]S[_]NP': 307, '6[_]SBAR[+]S[_]VP': 308, '6[_]SBAR[_]VP': 309, '6[_]SBAR[_]WHADVP': 310, '6[_]SBAR[_]WHNP': 311, '6[_]S[+]VP': 312, '6[_]S[_]ADVP': 313, '6[_]S[_]NP': 314, '6[_]S[_]VP': 315, '6[_]VP': 316, '6[_]VP[_]ADVP': 317, '6[_]VP[_]NP': 318, '6[_]VP[_]PP': 319, '6[_]VP[_]PRT': 320, '7[_]ADJP': 321, '7[_]ADJP[_]ADJP': 322, '7[_]ADVP': 323, '7[_]ADVP[_]ADVP': 324, '7[_]FRAG': 325, '7[_]NP': 326, '7[_]NP[+]NP': 327, '7[_]NP[+]QP': 328, '7[_]NP[_]NP': 329, '7[_]NX': 330, '7[_]NX[_]NP': 331, '7[_]PP': 332, '7[_]PRN': 333, '7[_]PRN[_]NP': 334, '7[_]QP': 335, '7[_]S': 336, '7[_]SBAR': 337, '7[_]SBAR[+]S': 338, '7[_]SBAR[+]SINV': 339, '7[_]SBAR[+]SINV[_]NP': 340, '7[_]SBAR[+]S[+]VP': 341, '7[_]SBAR[+]S[_]NP': 342, '7[_]SBAR[_]WHADVP': 343, '7[_]SBAR[_]WHNP': 344, '7[_]S[+]VP': 345, '7[_]S[+]VP[_]NP': 346, '7[_]S[_]ADVP': 347, '7[_]S[_]NP': 348, '7[_]VP': 349, '7[_]VP[_]ADVP': 350, '7[_]VP[_]NP': 351, '7[_]VP[_]VP': 352, '7[_]WHPP': 353, '8[_]ADJP': 354, '8[_]ADVP': 355, '8[_]NAC': 356, '8[_]NP': 357, '8[_]NP[+]QP': 358, '8[_]NP[_]NP': 359, '8[_]NX': 360, '8[_]PP': 361, '8[_]PRN': 362, '8[_]QP': 363, '8[_]S': 364, '8[_]SBAR': 365, '8[_]SBAR[+]S': 366, '8[_]SBAR[+]S[+]VP': 367, '8[_]SBAR[_]WHADVP': 368, '8[_]SBAR[_]WHNP': 369, '8[_]S[+]VP': 370, '8[_]S[+]VP[_]ADVP': 371, '8[_]S[_]ADVP': 372, '8[_]S[_]NP': 373, '8[_]S[_]NP[+]NP': 374, '8[_]S[_]S[+]VP': 375, '8[_]UCP': 376, '8[_]VP': 377, '8[_]VP[_]ADJP': 378, '8[_]VP[_]ADVP': 379, '8[_]VP[_]NP': 380, '8[_]VP[_]PRT': 381, '8[_]VP[_]VP': 382, '8[_]WHPP': 383, '9[_]ADJP': 384, '9[_]ADJP[+]QP': 385, '9[_]NAC': 386, '9[_]NP': 387, '9[_]NP[+]QP': 388, '9[_]NP[_]NP': 389, '9[_]PP': 390, '9[_]PRN': 391, '9[_]PRN[_]VP': 392, '9[_]QP': 393, '9[_]S': 394, '9[_]SBAR': 395, '9[_]SBAR[+]S': 396, '9[_]SBAR[_]ADVP': 397, '9[_]SBAR[_]WHNP': 398, '9[_]S[+]PP': 399, '9[_]S[+]VP': 400, '9[_]S[+]VP[_]NP': 401, '9[_]S[+]VP[_]VP': 402, '9[_]S[_]NP': 403, '9[_]VP': 404, '9[_]VP[_]ADJP': 405, '9[_]VP[_]ADVP': 406, '9[_]VP[_]NP': 407, '9[_]VP[_]PRT': 408, '9[_]WHPP': 409}


==> does this work for saved models? (i.e. load and test)