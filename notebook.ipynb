{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine Task dataclass to add ids to differentiate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import logger\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import ClassLabel, load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from typing import List, Optional, Union\n",
    "import torch \n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers import EvalPrediction\n",
    "import random\n",
    "from logger import logger\n",
    "from transformers import DataCollatorWithPadding, default_data_collator\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Task:\n",
    "    id: int\n",
    "    name: str\n",
    "    type: str\n",
    "    num_labels: int"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer and label to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "tasks=[\"token_classification\", \"seq_classification\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load token classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token_classification_dataset(task_id, tokenizer, model_args, data_args, training_config_args, training_args):\n",
    "    # load dataset from glue task\n",
    "    if data_args.task_name is not None:\n",
    "        raw_datasets = load_dataset(\n",
    "            \"glue\",\n",
    "            data_args.task_name,\n",
    "            cache_dir = model_args.cache_dir,\n",
    "            use_auth_token = True if model_args.use_auth_token else None,\n",
    "        )\n",
    "\n",
    "    # load dataset from hub\n",
    "    elif data_args.dataset_name is not None:\n",
    "        raw_datasets = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "    \n",
    "    # load dataset from local files\n",
    "    else:\n",
    "        # build the data_files object\n",
    "        data_files   = {\"train\": data_args.train_file, \"validation\": data_args.validation_file}\n",
    "        if training_args.do_predict:\n",
    "            if data_args.test_file is not None:\n",
    "                data_files[\"test\"] = data_args.test_file\n",
    "            else:\n",
    "                raise ValueError(\"Need a test file for `do_predict`.\")\n",
    "        \n",
    "        # load\n",
    "        if data_args.train_file.endswith(\".csv\"):\n",
    "            raw_datasets = load_dataset(\n",
    "                \"csv\",\n",
    "                data_files=data_files,\n",
    "                cache_dir=model_args.cache_dir,\n",
    "                use_auth_token=True if model_args.use_auth_token else None,\n",
    "            )\n",
    "        else:\n",
    "            raw_datasets = load_dataset(\n",
    "                \"json\",\n",
    "                data_files=data_files,\n",
    "                cache_dir=model_args.cache_dir,\n",
    "                use_auth_token=True if model_args.use_auth_token else None,\n",
    "            )\n",
    "    \n",
    "    # get the column names and featuers\n",
    "    if training_args.do_train:\n",
    "        column_names = raw_datasets[\"train\"].column_names\n",
    "        features = raw_datasets[\"train\"].features\n",
    "    else:\n",
    "        column_names = raw_datasets[\"validation\"].column_names\n",
    "        features = raw_datasets[\"validation\"].features\n",
    "\n",
    "    # get the tokens column name\n",
    "    if data_args.text_column_name is not None:\n",
    "        text_column_name = data_args.text_column_name\n",
    "    elif \"tokens\" in column_names:\n",
    "        # default column for token is 'tokens'\n",
    "        text_column_name = \"tokens\"\n",
    "    else:\n",
    "        text_column_name = column_names[0]\n",
    "\n",
    "    # get the labels column name\n",
    "    if data_args.label_column_name is not None:\n",
    "        label_column_name = data_args.label_column_name\n",
    "    elif f\"{data_args.task_name}_tags\" in column_names:\n",
    "        # default column for labels is 'xxx_tags'\n",
    "        label_column_name = f\"{data_args.task_name}_tags\"\n",
    "    else:\n",
    "        label_column_name = column_names[-1]\n",
    "    \n",
    "    # print sample of raw datasets with column names\n",
    "    # get label vocabulary (train + validation)\n",
    "    def get_label_list(labels):\n",
    "        unique_labels = set()\n",
    "        for label in labels:\n",
    "            unique_labels = unique_labels | set(label)\n",
    "        label_list = list(unique_labels)\n",
    "        label_list.sort()\n",
    "        return label_list\n",
    "    \n",
    "    label_list_train = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
    "    label_list_val   = get_label_list(raw_datasets[\"validation\"][label_column_name])\n",
    "    label_list = list(set(label_list_train) | set(label_list_val))\n",
    "    \n",
    "    num_labels = len(label_list)\n",
    "    label_to_id = {label_list[i]: i for i in range(len(label_list))}\n",
    "    \n",
    "    # set padding and truncation\n",
    "    padding = \"max_length\" if training_config_args.pad_to_max_length else False\n",
    "\n",
    "    # preprocess the dataset\n",
    "    def preprocess_function(data):\n",
    "        # tokenize the input text\n",
    "        tokenized_inputs = tokenizer(data[text_column_name], padding=padding,\n",
    "            truncation=True, max_length=training_config_args.max_seq_length, is_split_into_words=True)\n",
    "        \n",
    "        # align the labels with the input tokens\n",
    "        labels = []\n",
    "        for i, label in enumerate(data[label_column_name]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                # special tokens have None as word id; we set the label to -100 so the loss ignores them\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                \n",
    "                # set label for the first token of each word\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label_to_id[label[word_idx]])\n",
    "                \n",
    "                # for the first token, we set either the current label or -100 depending on the label_all_tokens flag\n",
    "                else:\n",
    "                    if data_args.label_all_tokens:\n",
    "                        label_ids.append([label_to_id[label[word_idx]]])\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        # set the labels column and taskids column\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        tokenized_inputs[\"task_ids\"] = [task_id] * len(tokenized_inputs[\"labels\"])\n",
    "        return tokenized_inputs\n",
    "    \n",
    "    # tokenize using torch multiprocessing\n",
    "    columns_to_drop = raw_datasets[\"train\"].column_names    \n",
    "    with training_args.main_process_first(desc=\"dataset map pre-processing\"):\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            preprocess_function,\n",
    "            batched              = True,\n",
    "            num_proc             = training_config_args.preprocessing_num_workers,\n",
    "            load_from_cache_file = not training_config_args.overwrite_cache,\n",
    "            remove_columns       = columns_to_drop,\n",
    "            desc                 = \"Running tokenizer on dataset (tok_class)\"\n",
    "        )\n",
    "\n",
    "    task_info = Task(\n",
    "        id          = task_id,\n",
    "        name        = \"token_classification-id=\"+str(task_id),\n",
    "        num_labels  = num_labels,\n",
    "        type        = \"token_classification\",\n",
    "    )\n",
    "\n",
    "    return tokenized_datasets[\"train\"], tokenized_datasets[\"validation\"], task_info,\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sequence classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seq_classification_dataset(task_id, tokenizer, model_args, data_args, training_config_args, training_args):\n",
    "    # load dataset from glue task\n",
    "    if data_args.task_name is not None:\n",
    "        raw_datasets = load_dataset(\n",
    "            \"glue\",\n",
    "            data_args.task_name,\n",
    "            cache_dir = model_args.cache_dir,\n",
    "            use_auth_token = True if model_args.use_auth_token else None,\n",
    "        )\n",
    "\n",
    "    # load dataset from hub\n",
    "    elif data_args.dataset_name is not None:\n",
    "        raw_datasets = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "            use_auth_token=True if model_args.use_auth_token else None,\n",
    "        )\n",
    "    \n",
    "    # load dataset from local files\n",
    "    else:\n",
    "        # build the data_files object\n",
    "        data_files = {\"train\": data_args.train_file, \"validation\": data_args.validation_file}\n",
    "        if training_args.do_predict:\n",
    "            if data_args.test_file is not None:\n",
    "                data_files[\"test\"] = data_args.test_file\n",
    "            else:\n",
    "                raise ValueError(\"Need a test file for `do_predict`.\")\n",
    "        \n",
    "        # load\n",
    "        if data_args.train_file.endswith(\".csv\"):\n",
    "            raw_datasets = load_dataset(\n",
    "                \"csv\",\n",
    "                data_files=data_files,\n",
    "                cache_dir=model_args.cache_dir,\n",
    "                use_auth_token=True if model_args.use_auth_token else None,\n",
    "            )\n",
    "        else:\n",
    "            raw_datasets = load_dataset(\n",
    "                \"json\",\n",
    "                data_files=data_files,\n",
    "                cache_dir=model_args.cache_dir,\n",
    "                use_auth_token=True if model_args.use_auth_token else None,\n",
    "            )\n",
    "\n",
    "    # get the column names and featuers\n",
    "    if training_args.do_train:\n",
    "        column_names = raw_datasets[\"train\"].column_names\n",
    "        features = raw_datasets[\"train\"].features\n",
    "    else:\n",
    "        column_names = raw_datasets[\"validation\"].column_names\n",
    "        features = raw_datasets[\"validation\"].features\n",
    "\n",
    "    # get the text (or texts!!!) column name\n",
    "    if data_args.task_name is not None:\n",
    "        # get the sentence1_key and sentence2_key from the task name\n",
    "        sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n",
    "    else:\n",
    "        non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n",
    "        # check if in our non_label_column_names we have sentence1 and sentence2\n",
    "        if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n",
    "            sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n",
    "        # check if in our non_label_column_names we have text and text2\n",
    "        elif \"text\" in non_label_column_names:\n",
    "            sentence1_key, sentence2_key = \"text\", None\n",
    "        # assign default numbers\n",
    "        else:\n",
    "            if len(non_label_column_names) >= 2:\n",
    "                sentence1_key, sentence2_key = non_label_column_names[:2]\n",
    "            else:\n",
    "                sentence1_key, sentence2_key = non_label_column_names[0], None\n",
    "\n",
    "    # check if we are in a regression problem (stsb tasks or label is a float)\n",
    "    if (data_args.task_name is not None and data_args.task_name==\"stsb\") or \\\n",
    "        (raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]):\n",
    "        num_labels = 1    \n",
    "        label_to_id = None\n",
    "    else:\n",
    "        # get the labels column name\n",
    "        if data_args.label_column_name is not None:\n",
    "            label_column_name = data_args.label_column_name\n",
    "        elif \"label\" in column_names:\n",
    "            label_column_name = \"label\"\n",
    "        elif f\"{data_args.task_name}_label\" in column_names:\n",
    "            label_column_name = f\"{data_args.task_name}_label\"\n",
    "        else:\n",
    "            label_column_name = column_names[-1]\n",
    "        \n",
    "        # get label vocabulary (train + validation)\n",
    "        def get_label_list(labels):\n",
    "            unique_labels = set()\n",
    "            for label in labels:\n",
    "                unique_labels.add(label)\n",
    "            label_list = list(unique_labels)\n",
    "            label_list.sort()\n",
    "            return label_list\n",
    "\n",
    "        label_list_train = get_label_list(raw_datasets[\"train\"][label_column_name])\n",
    "        label_list_val   = get_label_list(raw_datasets[\"validation\"][label_column_name])\n",
    "        label_list = list(set(label_list_train) | set(label_list_val))\n",
    "\n",
    "        num_labels = len(label_list)\n",
    "        label_to_id = {i: i for i in range(len(label_list))}\n",
    "\n",
    "    # get padding and max sequence length\n",
    "    if training_config_args.pad_to_max_length:\n",
    "        padding = \"max_length\"\n",
    "    else:\n",
    "        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "        padding = False\n",
    "\n",
    "    if training_config_args.max_seq_length > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The max_seq_length passed ({training_config_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length = min(training_config_args.max_seq_length, tokenizer.model_max_length)\n",
    "    \n",
    "    # preprocess the dataset\n",
    "    def preprocess_function(data):\n",
    "        # tokenize the input text or texts\n",
    "        args = (\n",
    "            (data[sentence1_key],) if sentence2_key is None else (data[sentence1_key], data[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "        \n",
    "        # map labels to ids (note that we are outputting it in label*S* instead of label)\n",
    "        if label_to_id is not None and \"label\" in data:\n",
    "            result[\"labels\"] = [(label_to_id[l] if l != -1 else -1) for l in data[\"label\"]]\n",
    "        \n",
    "        # pad the labels to max seq length\n",
    "        result[\"labels\"]  = [[l] + [-100] * (max_seq_length - 1) for l in result[\"labels\"]]\n",
    "        \n",
    "        # set the task ids\n",
    "        result[\"task_ids\"] = [task_id] * len(result[\"labels\"])\n",
    "        return result\n",
    "    \n",
    "    # tokenize using torch multiprocessing\n",
    "    columns_to_drop = raw_datasets[\"train\"].column_names    \n",
    "    with training_args.main_process_first(desc=\"dataset map pre-processing\"):\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            preprocess_function,\n",
    "            batched              = True,\n",
    "            num_proc             = training_config_args.preprocessing_num_workers,\n",
    "            load_from_cache_file = not training_config_args.overwrite_cache,\n",
    "            remove_columns       = columns_to_drop,\n",
    "            desc                 = \"Running tokenizer on dataset (seq_class)\",\n",
    "        )\n",
    "        \n",
    "    task_info = Task(\n",
    "        id          = task_id,\n",
    "        name        = \"seq_classification-id=\"+str(task_id),\n",
    "        num_labels  = num_labels,\n",
    "        type        = \"seq_classification\",\n",
    "    )\n",
    "\n",
    "    return tokenized_datasets[\"train\"], tokenized_datasets[\"validation\"], task_info\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(tokenizer, model_args, data_args, training_config_args, training_args):\n",
    "    ### Problem: the datasets are currently being passed to both load_seq functions.\n",
    "    ### We should edit the DataTrainingArguments in order to have different datasets for each task\n",
    "    print(data_args)\n",
    "    \n",
    "    idx = 0\n",
    "    tasks = []\n",
    "    for task in data_args:\n",
    "        print(\"Loading dataset for\",task)\n",
    "        if task.task_type == \"seq_classification\":\n",
    "            (train, val, task_info) = load_seq_classification_dataset(idx, tokenizer, model_args, task, training_config_args, training_args)\n",
    "        \n",
    "        elif task.task_type == \"token_classification\":\n",
    "            (train, val, task_info) = load_token_classification_dataset(idx, tokenizer, model_args, task, training_config_args, training_args)\n",
    "        \n",
    "        # print train columns\n",
    "        print(train.column_names)\n",
    "        tasks.append({\"train\":train, \"val\":val, \"task_info\":task_info})\n",
    "        idx += 1\n",
    "\n",
    "    # merge datasets\n",
    "    train_dataset_df    = tasks[0][\"train\"].to_pandas()\n",
    "    validation_dataset  = [tasks[0][\"val\"]]\n",
    "    tasks_info          = [tasks[0][\"task_info\"]]\n",
    "    for i in range(1, len(tasks)):\n",
    "        train_dataset_df    = pd.concat([train_dataset_df, tasks[i][\"train\"].to_pandas()], ignore_index=True)\n",
    "        validation_dataset.append(tasks[i][\"val\"])\n",
    "        tasks_info.append(tasks[i][\"task_info\"])\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_pandas(train_dataset_df)\n",
    "    train_dataset.shuffle(seed=123)\n",
    "\n",
    "    dataset = datasets.DatasetDict({\"train\": train_dataset, \"validation\": validation_dataset})\n",
    "    \n",
    "    return tasks_info, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_p = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output, labels=None, attention_mask=None, **kwargs):\n",
    "        sequence_output_dropout = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output_dropout)\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            labels = labels.long()\n",
    "\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss,\n",
    "                    labels.view(-1),\n",
    "                    torch.tensor(loss_fct.ignore_index).type_as(labels),\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "# based on transformers.models.bert.modeling_bert.BertForSequenceClassification\n",
    "class SequenceClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, sequence_output, pooled_output, labels=None, **kwargs):\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if labels.dim() != 1:\n",
    "                # Remove padding\n",
    "                labels = labels[:, 0]\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.num_labels), labels.long().view(-1)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()\n",
    "\n",
    "# multi-task model\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, encoder_name_or_path, tasks):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name_or_path)\n",
    "        self.output_heads = nn.ModuleDict()\n",
    "\n",
    "        for task in tasks:\n",
    "            decoder = self._create_output_head(self.encoder.config.hidden_size, task)\n",
    "            self.output_heads[str(task.id)] = decoder\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_output_head(encoder_hidden_size, task):\n",
    "        if task.type == \"seq_classification\":\n",
    "            return SequenceClassificationHead(encoder_hidden_size, task.num_labels)\n",
    "        elif task.type == \"token_classification\":\n",
    "            return TokenClassificationHead(encoder_hidden_size, task.num_labels)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "            inputs_embeds=None, labels=None, task_ids=None, **kwargs):\n",
    "\n",
    "            # get the output from the pretrained transformer\n",
    "            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "                position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n",
    "\n",
    "            sequence_output, pooled_output = outputs[:2]\n",
    "            unique_task_ids_list = torch.unique(task_ids).tolist()\n",
    "\n",
    "            loss_list = []\n",
    "            logits = None\n",
    "\n",
    "            # run the output head for each task\n",
    "            for unique_task_id in unique_task_ids_list:\n",
    "                task_id_filter = (task_ids == unique_task_id)\n",
    "                logits, task_loss = self.output_heads[str(unique_task_id)].forward(\n",
    "                    sequence_output[task_id_filter],\n",
    "                    pooled_output[task_id_filter],\n",
    "                    labels=None if labels is None else labels[task_id_filter],\n",
    "                    attention_mask=attention_mask[task_id_filter],\n",
    "                )\n",
    "\n",
    "                if labels is not None:\n",
    "                    loss_list.append(task_loss)\n",
    "\n",
    "            outputs = (logits, outputs[2:])\n",
    "\n",
    "            # compute the global loss of the multi-task model\n",
    "            if loss_list:\n",
    "                loss = torch.stack(loss_list)\n",
    "                outputs = (loss.mean(),) + outputs\n",
    "\n",
    "            return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "\n",
    "    if preds.ndim == 2:\n",
    "        # Token classification\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "    elif preds.ndim == 3:\n",
    "        # Sequence classification\n",
    "        metric = load_metric(\"seqeval\")\n",
    "\n",
    "        predictions = np.argmax(preds, axis=2)\n",
    "\n",
    "        true_predictions = [\n",
    "            [f\"tag-idx-{p}\" for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, p.label_ids)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [f\"tag-idx-{l}\" for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, p.label_ids)\n",
    "        ]\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        results = metric.compute(\n",
    "            predictions=true_predictions, references=true_labels\n",
    "        )\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments related to the training data for each task.\n",
    "    \"\"\"\n",
    "\n",
    "## dataset loading\n",
    "    task_type: str = field(\n",
    "        default  = None,\n",
    "        metadata = {\"help\": \"The type of task \"+ \", \".join(tasks) },\n",
    "    )\n",
    "    \n",
    "    task_name: Optional[str] = field(\n",
    "        default  = None,\n",
    "        metadata = {\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n",
    "    )\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default  = None, \n",
    "        metadata = {\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default  = None, \n",
    "        metadata = {\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "\n",
    "    train_file: Optional[str] = field(\n",
    "        default  = None, \n",
    "        metadata = {\"help\": \"A csv or a json file containing the training data.\"}\n",
    "    )\n",
    "\n",
    "    validation_file: Optional[str] = field(\n",
    "        default  = None, \n",
    "        metadata = {\"help\": \"A csv or a json file containing the validation data.\"}\n",
    "    )\n",
    "\n",
    "    test_file: Optional[str] = field(\n",
    "        default  = None, \n",
    "        metadata = {\"help\": \"A csv or a json file containing the test data.\"}\n",
    "    )\n",
    "\n",
    "## dataset info\n",
    "\n",
    "    text_column_name: Optional[str] = field(\n",
    "        default  = None, \n",
    "        metadata = {\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "    \n",
    "    label_column_name: Optional[str] = field(\n",
    "        default  = None, \n",
    "        metadata = {\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n",
    "    )\n",
    "\n",
    "    label_all_tokens: Optional[bool] = field(\n",
    "        default  = False,\n",
    "        metadata = {\n",
    "            \"help\": (\n",
    "                \"Whether to put the label for one word on all tokens \"\n",
    "                \"where the word is part of (for token classification).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    ignore_columns: Optional[List[str]] = field(\n",
    "        default_factory = list,\n",
    "        metadata        = {\"help\": \"A list of columns to ignore from the file (a csv or JSON file).\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # check that we have either a dataset_name or dataset files\n",
    "        \n",
    "        if self.task_name is not None:\n",
    "            self.task_name = self.task_name.lower()\n",
    "            if self.task_name not in task_to_keys.keys():\n",
    "                raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n",
    "        \n",
    "        elif self.dataset_name is not None:\n",
    "            pass\n",
    "        \n",
    "        elif self.train_file is None or self.validation_file is None:\n",
    "            raise ValueError(\"Need either a GLUE task, a training/validation file or a dataset name.\")\n",
    "        \n",
    "        else:\n",
    "            # check valid extension\n",
    "            train_extension = self.train_file.split(\".\")[-1]\n",
    "            assert train_extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "\n",
    "            # check that the validation file has the same extension\n",
    "            validation_extension = self.validation_file.split(\".\")[-1]\n",
    "            assert (\n",
    "                validation_extension == train_extension\n",
    "            ), \"`validation_file` should have the same extension (csv or json) as `train_file`.\"\n",
    "            \n",
    "            # check if we have a test file and if it has the same extension\n",
    "            if self.test_file is not None:\n",
    "                test_extension = self.test_file.split(\".\")[-1]\n",
    "                assert (\n",
    "                    test_extension == train_extension\n",
    "                ), \"`test_file` should have the same extension (csv or json) as `train_file`.\"\n",
    "\n",
    "                \n",
    "@dataclass\n",
    "class TrainingConfigArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to the training configuration.\n",
    "    \"\"\"\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default  = 1,\n",
    "        metadata = {\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "\n",
    "    pad_to_max_length: bool = field(\n",
    "        default  = True,\n",
    "        metadata = {\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to `max_seq_length`. \"\n",
    "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    max_seq_length: int = field(\n",
    "        default  = 128,\n",
    "        metadata = {\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    overwrite_cache: bool = field(\n",
    "        default  = False, \n",
    "        metadata = {\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
    "    )\n",
    "\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default  = None,\n",
    "        metadata = {\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default  = None,\n",
    "        metadata = {\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default  = None,\n",
    "        metadata = {\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    data_cache_dir: Optional[str] = field(\n",
    "        default  = None,\n",
    "        metadata = {\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    encoder_name_or_path: str = field(\n",
    "        metadata = {\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "\n",
    "    config_name: Optional[str] = field(\n",
    "        default  = None, \n",
    "        metadata = {\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default  = None, \n",
    "        metadata = {\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default  = None,\n",
    "        metadata = {\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default  = True,\n",
    "        metadata = {\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default  = \"main\",\n",
    "        metadata = {\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default  = False,\n",
    "        metadata = {\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    ignore_mismatched_sizes: bool = field(\n",
    "        default  = False,\n",
    "        metadata = {\"help\": \"Will enable to load a pretrained model whose head dimensions are different.\"},\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_args, data_args, training_config_args, training_args):\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if (\n",
    "        os.path.isdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif (\n",
    "            last_checkpoint is not None and training_args.resume_from_checkpoint is None\n",
    "        ):\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.encoder_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    # load dataset/tasks and generate the mtl model\n",
    "    tasks, raw_datasets = load_datasets(tokenizer, model_args, data_args, training_config_args, training_args)\n",
    "    model = MultiTaskModel(model_args.encoder_name_or_path, tasks)\n",
    "\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in raw_datasets:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = raw_datasets[\"train\"]\n",
    "        \n",
    "        if training_config_args.max_train_samples is not None:\n",
    "            train_dataset = train_dataset.select(range(training_config_args.max_train_samples))\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        if (\n",
    "            \"validation\" not in raw_datasets\n",
    "            and \"validation_matched\" not in raw_datasets\n",
    "        ):\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_datasets = raw_datasets[\"validation\"]\n",
    "        \n",
    "        if training_config_args.max_eval_samples is not None:\n",
    "            new_ds = []\n",
    "            for ds in eval_datasets:\n",
    "                new_ds.append(ds.select(range(training_config_args.max_eval_samples)))\n",
    "\n",
    "            eval_datasets = new_ds\n",
    "\n",
    "    # Log a few random samples from the training set:\n",
    "    print(train_dataset)\n",
    "    if training_args.do_train:\n",
    "        for index in random.sample(range(len(train_dataset)), 3):\n",
    "            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None\n",
    "    )\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset if training_args.do_train else None,\n",
    "        compute_metrics = compute_metrics,\n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = data_collator,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    if training_args.do_train:\n",
    "        checkpoint = None\n",
    "        if training_args.resume_from_checkpoint is not None:\n",
    "            checkpoint = training_args.resume_from_checkpoint\n",
    "        elif last_checkpoint is not None:\n",
    "            checkpoint = last_checkpoint\n",
    "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        metrics = train_result.metrics\n",
    "        max_train_samples = (\n",
    "            training_config_args.max_train_samples\n",
    "            if training_config_args.max_train_samples is not None\n",
    "            else len(train_dataset)\n",
    "        )\n",
    "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "\n",
    "    # Evaluation\n",
    "    if training_args.do_eval:\n",
    "\n",
    "        for eval_dataset, task in zip(eval_datasets, tasks):\n",
    "            logger.info(f\"*** Evaluate {task} ***\")\n",
    "            data_collator = None\n",
    "            if task.type == \"token_classification\":\n",
    "                data_collator = DataCollatorForTokenClassification(\n",
    "                    tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None\n",
    "                )\n",
    "            else:\n",
    "                if training_config_args.pad_to_max_length:\n",
    "                    data_collator = default_data_collator\n",
    "                elif training_args.fp16:\n",
    "                    data_collator = DataCollatorWithPadding(\n",
    "                        tokenizer, pad_to_multiple_of=8\n",
    "                    )\n",
    "                else:\n",
    "                    data_collator = None\n",
    "\n",
    "            trainer.data_collator = data_collator\n",
    "            metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "            max_eval_samples = (\n",
    "                training_config_args.max_eval_samples\n",
    "                if training_config_args.max_eval_samples is not None\n",
    "                else len(eval_datasets)\n",
    "            )\n",
    "            metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_datasets))\n",
    "\n",
    "            trainer.log_metrics(\"eval\", metrics)\n",
    "            trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Apr/2023 12:05:14] WARNING - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "[DataTrainingArguments(task_type='token_classification', task_name=None, dataset_name='conll2003', dataset_config_name=None, train_file=None, validation_file=None, test_file=None, text_column_name='tokens', label_column_name='ner_tags', label_all_tokens=False, ignore_columns=[]), DataTrainingArguments(task_type='token_classification', task_name=None, dataset_name='universal_dependencies', dataset_config_name='en_ewt', train_file=None, validation_file=None, test_file=None, text_column_name='tokens', label_column_name='head', label_all_tokens=False, ignore_columns=[]), DataTrainingArguments(task_type='token_classification', task_name=None, dataset_name='universal_dependencies', dataset_config_name='en_ewt', train_file=None, validation_file=None, test_file=None, text_column_name='tokens', label_column_name='upos', label_all_tokens=False, ignore_columns=[]), DataTrainingArguments(task_type='seq_classification', task_name=None, dataset_name='tweet_eval', dataset_config_name='emotion', train_file=None, validation_file=None, test_file=None, text_column_name='text', label_column_name='label', label_all_tokens=False, ignore_columns=[])]\n",
      "Loading dataset for DataTrainingArguments(task_type='token_classification', task_name=None, dataset_name='conll2003', dataset_config_name=None, train_file=None, validation_file=None, test_file=None, text_column_name='tokens', label_column_name='ner_tags', label_all_tokens=False, ignore_columns=[])\n",
      "[14/Apr/2023 12:05:16] WARNING - Found cached dataset conll2003 (/home/poli/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dd90108cfb4fff99ee8f4e04b59748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Apr/2023 12:05:17] WARNING - Loading cached processed dataset at /home/poli/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-be2f3576c098a52d.arrow\n",
      "[14/Apr/2023 12:05:17] WARNING - Loading cached processed dataset at /home/poli/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-d74fd1ce17452ee2.arrow\n",
      "[14/Apr/2023 12:05:17] WARNING - Loading cached processed dataset at /home/poli/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-fba76b75865f9467.arrow\n",
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'task_ids']\n",
      "Loading dataset for DataTrainingArguments(task_type='token_classification', task_name=None, dataset_name='universal_dependencies', dataset_config_name='en_ewt', train_file=None, validation_file=None, test_file=None, text_column_name='tokens', label_column_name='head', label_all_tokens=False, ignore_columns=[])\n",
      "[14/Apr/2023 12:05:21] WARNING - Found cached dataset universal_dependencies (/home/poli/.cache/huggingface/datasets/universal_dependencies/en_ewt/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dd43cd0a0445c6ad687796ac799e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c936efde3c444b29a5dc900c3fb922c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (tok_class):   0%|          | 0/12543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa98af1b178545f896b03152f3804055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (tok_class):   0%|          | 0/2002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db207f373a94afda73442174c5e5b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (tok_class):   0%|          | 0/2077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'task_ids']\n",
      "Loading dataset for DataTrainingArguments(task_type='token_classification', task_name=None, dataset_name='universal_dependencies', dataset_config_name='en_ewt', train_file=None, validation_file=None, test_file=None, text_column_name='tokens', label_column_name='upos', label_all_tokens=False, ignore_columns=[])\n",
      "[14/Apr/2023 12:05:27] WARNING - Found cached dataset universal_dependencies (/home/poli/.cache/huggingface/datasets/universal_dependencies/en_ewt/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557135e596a2470b8354bac2af2a5a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d236738316944f8db3b7082d7df5f8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset (tok_class):   0%|          | 0/12543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Apr/2023 12:05:29] WARNING - Loading cached processed dataset at /home/poli/.cache/huggingface/datasets/universal_dependencies/en_ewt/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7/cache-5c589620bf4be8f0.arrow\n",
      "[14/Apr/2023 12:05:29] WARNING - Loading cached processed dataset at /home/poli/.cache/huggingface/datasets/universal_dependencies/en_ewt/2.7.0/1ac001f0e8a0021f19388e810c94599f3ac13cc45d6b5b8c69f7847b2188bdf7/cache-cc45826c0400ccb9.arrow\n",
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'task_ids']\n",
      "Loading dataset for DataTrainingArguments(task_type='seq_classification', task_name=None, dataset_name='tweet_eval', dataset_config_name='emotion', train_file=None, validation_file=None, test_file=None, text_column_name='text', label_column_name='label', label_all_tokens=False, ignore_columns=[])\n",
      "[14/Apr/2023 12:05:31] WARNING - Found cached dataset tweet_eval (/home/poli/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd85813b529745e195acb905cb8e42b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/Apr/2023 12:05:32] WARNING - Loading cached processed dataset at /home/poli/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-c6ab0bf7a1e111b8.arrow\n",
      "[14/Apr/2023 12:05:32] WARNING - Loading cached processed dataset at /home/poli/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-c311697780004e78.arrow\n",
      "[14/Apr/2023 12:05:32] WARNING - Loading cached processed dataset at /home/poli/.cache/huggingface/datasets/tweet_eval/emotion/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-6419657289403b28.arrow\n",
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'task_ids']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:3024] 2023-04-14 12:05:34,993 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'task_ids'],\n",
      "    num_rows: 42384\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poli/.local/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aa856b6dec4ddb81c5b26ebcad2792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3975 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:280] 2023-04-14 12:05:35,152 >> You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 52\u001b[0m\n\u001b[1;32m     43\u001b[0m data_args_4 \u001b[39m=\u001b[39m DataTrainingArguments(\n\u001b[1;32m     44\u001b[0m                     task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mseq_classification\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m                     dataset_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtweet_eval\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m                     dataset_config_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m                     text_column_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     48\u001b[0m                     label_column_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m training_config_args \u001b[39m=\u001b[39m TrainingConfigArguments(max_seq_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m main(model_args, [data_args_1, data_args_2, data_args_3, data_args_4], training_config_args, training_args)\n",
      "Cell \u001b[0;32mIn[21], line 109\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_args, data_args, training_config_args, training_args)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39melif\u001b[39;00m last_checkpoint \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     checkpoint \u001b[39m=\u001b[39m last_checkpoint\n\u001b[0;32m--> 109\u001b[0m train_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49mcheckpoint)\n\u001b[1;32m    110\u001b[0m metrics \u001b[39m=\u001b[39m train_result\u001b[39m.\u001b[39mmetrics\n\u001b[1;32m    111\u001b[0m max_train_samples \u001b[39m=\u001b[39m (\n\u001b[1;32m    112\u001b[0m     training_config_args\u001b[39m.\u001b[39mmax_train_samples\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m training_config_args\u001b[39m.\u001b[39mmax_train_samples \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(train_dataset)\n\u001b[1;32m    115\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1901\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1904\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1905\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1906\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1907\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1908\u001b[0m ):\n\u001b[1;32m   1909\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2642\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2644\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2645\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2647\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2648\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2677\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2676\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2677\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2678\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 96\u001b[0m, in \u001b[0;36mMultiTaskModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, position_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, head_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m         inputs_embeds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, task_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m         \u001b[39m# get the output from the pretrained transformer\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m     97\u001b[0m             position_ids\u001b[39m=\u001b[39;49mposition_ids, head_mask\u001b[39m=\u001b[39;49mhead_mask, inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds)\n\u001b[1;32m     99\u001b[0m         sequence_output, pooled_output \u001b[39m=\u001b[39m outputs[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    100\u001b[0m         unique_task_ids_list \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munique(task_ids)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1021\u001b[0m     embedding_output,\n\u001b[1;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1031\u001b[0m )\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    611\u001b[0m         hidden_states,\n\u001b[1;32m    612\u001b[0m         attention_mask,\n\u001b[1;32m    613\u001b[0m         layer_head_mask,\n\u001b[1;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    616\u001b[0m         past_key_value,\n\u001b[1;32m    617\u001b[0m         output_attentions,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    496\u001b[0m         hidden_states,\n\u001b[1;32m    497\u001b[0m         attention_mask,\n\u001b[1;32m    498\u001b[0m         head_mask,\n\u001b[1;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[1;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m    428\u001b[0m         head_mask,\n\u001b[1;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m         past_key_value,\n\u001b[1;32m    432\u001b[0m         output_attentions,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:353\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    350\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    352\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attention_scores, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    355\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:1811\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1807\u001b[0m         ret \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39m\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   1808\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n\u001b[0;32m-> 1811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msoftmax\u001b[39m(\u001b[39minput\u001b[39m: Tensor, dim: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, _stacklevel: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, dtype: Optional[DType] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m   1812\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies a softmax function.\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \n\u001b[1;32m   1814\u001b[0m \u001b[39m    Softmax is defined as:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1834\u001b[0m \n\u001b[1;32m   1835\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model_args = ModelArguments(encoder_name_or_path=\"bert-base-cased\")\n",
    "    training_args = TrainingArguments(\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        output_dir=\"./tmp/test\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        overwrite_output_dir=True,\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        logging_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        remove_unused_columns=False\n",
    "    )\n",
    "    # NER Tags\n",
    "    data_args_1 = DataTrainingArguments(\n",
    "                        task_type=\"token_classification\", \n",
    "                        dataset_name=\"conll2003\",\n",
    "                        text_column_name=\"tokens\",\n",
    "                        label_column_name=\"ner_tags\")\n",
    "    # Dependency head prediction\n",
    "    data_args_2 = DataTrainingArguments(\n",
    "                        task_type=\"token_classification\", \n",
    "                        dataset_name=\"universal_dependencies\", \n",
    "                        dataset_config_name=\"en_ewt\",\n",
    "                        text_column_name=\"tokens\",\n",
    "                        label_column_name=\"head\")\n",
    "    # POS Tags\n",
    "    data_args_3 = DataTrainingArguments(\n",
    "                        task_type=\"token_classification\", \n",
    "                        dataset_name=\"universal_dependencies\", \n",
    "                        dataset_config_name=\"en_ewt\",\n",
    "                        text_column_name=\"tokens\",\n",
    "                        label_column_name=\"upos\")\n",
    "    # Sentiment\n",
    "    data_args_4 = DataTrainingArguments(\n",
    "                        task_type=\"seq_classification\",\n",
    "                        dataset_name=\"tweet_eval\",\n",
    "                        dataset_config_name=\"emotion\",\n",
    "                        text_column_name=\"text\",\n",
    "                        label_column_name=\"label\")\n",
    "    \n",
    "\n",
    "    training_config_args = TrainingConfigArguments(max_seq_length=128)\n",
    "    main(model_args, [data_args_1, data_args_2, data_args_3, data_args_4], training_config_args, training_args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cfa9136a6dd62194a07e3de69c680f647f4acccfc6f18ef0b3fb404551b0b99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
